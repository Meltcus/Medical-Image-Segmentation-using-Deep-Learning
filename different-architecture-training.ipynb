{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9790462,"sourceType":"datasetVersion","datasetId":5999150}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import smp","metadata":{}},{"cell_type":"code","source":"pip install segmentation-models-pytorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-01T16:33:17.072127Z","iopub.execute_input":"2024-12-01T16:33:17.072424Z","iopub.status.idle":"2024-12-01T16:33:32.739716Z","shell.execute_reply.started":"2024-12-01T16:33:17.072396Z","shell.execute_reply":"2024-12-01T16:33:32.738681Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\nCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.25.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.3.0)\nCollecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\nCollecting timm==0.9.7 (from segmentation-models-pytorch)\n  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.0)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=fa533c13f4c41a4b1114359c18a46622f8e6134614ba4fe90b17bfcded8d85a2\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=9be1065061a27de72c36ba0f0343d0bb68f0d99adc00d77f2768704c0695622d\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.9\n    Uninstalling timm-1.0.9:\n      Successfully uninstalled timm-1.0.9\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt\n\nclass HuronDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = Path(image_dir)\n        self.mask_dir = Path(mask_dir)\n        self.transform = transform\n        self.mask_transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor()\n        ])\n        self.images = sorted(os.listdir(image_dir))\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_dir / self.images[idx]\n        mask_path = self.mask_dir / self.images[idx]\n        \n        image = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n        \n        if self.transform:\n            image = self.transform(image)\n        mask = self.mask_transform(mask)\n        \n        # Ensure mask is binary\n        mask = (mask > 0.5).float()\n        \n        return image, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T16:59:48.396654Z","iopub.execute_input":"2024-12-01T16:59:48.397115Z","iopub.status.idle":"2024-12-01T16:59:55.384204Z","shell.execute_reply.started":"2024-12-01T16:59:48.397079Z","shell.execute_reply":"2024-12-01T16:59:55.383405Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Eval and Metrics","metadata":{}},{"cell_type":"code","source":"def calculate_iou(outputs, targets, threshold=0.5):\n    outputs = (outputs > threshold).float()\n    targets = targets.float()\n    \n    intersection = (outputs * targets).sum(dim=(1, 2))\n    union = outputs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) - intersection\n    \n    iou = (intersection + 1e-6) / (union + 1e-6)\n    return iou.mean()\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_iou = 0\n    num_batches = len(dataloader)\n    \n    with torch.no_grad():\n        for images, masks in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            iou = calculate_iou(outputs, masks)\n            \n            total_loss += loss.item()\n            total_iou += iou.item()\n    \n    return total_loss / num_batches, total_iou / num_batches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n    best_val_iou = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_iou = 0.0\n        \n        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n            for images, masks in pbar:\n                images = images.to(device)\n                masks = masks.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                iou = calculate_iou(outputs, masks)\n                \n                loss.backward()\n                optimizer.step()\n                \n                running_loss += loss.item()\n                running_iou += iou.item()\n                \n                pbar.set_postfix({\n                    'loss': running_loss / (pbar.n + 1),\n                    'IoU': running_iou / (pbar.n + 1)\n                })\n        \n        # Validate\n        model.eval()\n        val_iou = 0\n        with torch.no_grad():\n            for images, masks in val_loader:\n                images = images.to(device)\n                masks = masks.to(device)\n                outputs = model(images)\n                val_iou += calculate_iou(outputs, masks).item()\n        \n        val_iou /= len(val_loader)\n        print(f'Validation IoU: {val_iou:.4f}')\n        \n        if val_iou > best_val_iou:\n            best_val_iou = val_iou\n            torch.save(model.state_dict(), 'best_model.pth')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Unet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize U-Net model\n    model = smp.Unet(\n        encoder_name=encoder_name,        # try different encoders\n        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n        in_channels=3,                  # model input channels\n        classes=1                      # model output channels\n    ).to(device)\n    \n    print(f\"Model: U-Net with {encoder_name} encoder\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T01:11:28.323348Z","iopub.execute_input":"2024-11-28T01:11:28.323861Z","iopub.status.idle":"2024-11-28T01:29:40.312818Z","shell.execute_reply.started":"2024-11-28T01:11:28.323818Z","shell.execute_reply":"2024-11-28T01:29:40.311740Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\nModel: U-Net with resnet34 encoder\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [03:08<00:00,  8.06it/s, loss=0.123, IoU=0.871]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8721\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [03:15<00:00,  7.77it/s, loss=0.09, IoU=0.894]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8953\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [03:15<00:00,  7.77it/s, loss=0.0871, IoU=0.896]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9023\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [03:15<00:00,  7.76it/s, loss=0.0846, IoU=0.899]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8991\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [03:15<00:00,  7.75it/s, loss=0.0848, IoU=0.899]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9001\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3598702032.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0840, Test IoU: 0.9016\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Unet with SCSE","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    model = smp.Unet(\n        encoder_name=encoder_name,        # try different encoders\n        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n        in_channels=3,                  # model input channels\n        classes=1,                      # model output channels\n        decoder_attention_type=\"scse\"   # spatial and channel squeeze & excitation\n    ).to(device)\n    \n    print(f\"Model: Attention U-Net with {encoder_name} encoder and SCSE attention\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T01:29:56.562045Z","iopub.execute_input":"2024-11-28T01:29:56.562449Z","iopub.status.idle":"2024-11-28T01:51:07.646332Z","shell.execute_reply.started":"2024-11-28T01:29:56.562391Z","shell.execute_reply":"2024-11-28T01:51:07.645041Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\nModel: Attention U-Net with resnet34 encoder and SCSE attention\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [03:52<00:00,  6.52it/s, loss=0.128, IoU=0.866]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8909\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [03:52<00:00,  6.53it/s, loss=0.0908, IoU=0.892]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9014\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [03:52<00:00,  6.53it/s, loss=0.086, IoU=0.897] \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8977\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [03:52<00:00,  6.53it/s, loss=0.0833, IoU=0.9]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9026\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [03:52<00:00,  6.54it/s, loss=0.0872, IoU=0.897]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9020\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1385039598.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0836, Test IoU: 0.9023\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Unet++","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    model = smp.UnetPlusPlus(\n        encoder_name=encoder_name,        # try different encoders\n        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n        in_channels=3,                  # model input channels\n        classes=1                     # model output channel\n    ).to(device)\n    \n    print(f\"Model: U-Net++ with {encoder_name}\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:03:22.333735Z","iopub.execute_input":"2024-12-01T18:03:22.334416Z","iopub.status.idle":"2024-12-01T18:39:53.223613Z","shell.execute_reply.started":"2024-12-01T18:03:22.334369Z","shell.execute_reply":"2024-12-01T18:39:53.222331Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\nModel: U-Net++ with resnet34\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [06:44<00:00,  3.76it/s, loss=0.116, IoU=0.874]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8813\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [06:42<00:00,  3.77it/s, loss=0.0897, IoU=0.896]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [06:42<00:00,  3.77it/s, loss=0.0895, IoU=0.895]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8956\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [06:42<00:00,  3.77it/s, loss=0.0842, IoU=0.9]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8945\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [06:43<00:00,  3.76it/s, loss=0.0809, IoU=0.903]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8954\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2377843142.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0835, Test IoU: 0.9012\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Unet++ with SCSE","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    model = smp.UnetPlusPlus(\n        encoder_name=encoder_name,        # try different encoders\n        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n        in_channels=3,                  # model input channels\n        classes=1,                      # model output channels\n        decoder_attention_type=\"scse\"   # spatial and channel squeeze & excitation\n    ).to(device)\n    \n    print(f\"Model: Attention U-Net++ with {encoder_name} encoder and SCSE attention\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:54:56.526850Z","iopub.execute_input":"2024-11-28T16:54:56.527242Z","iopub.status.idle":"2024-11-28T17:43:32.038783Z","shell.execute_reply.started":"2024-11-28T16:54:56.527210Z","shell.execute_reply":"2024-11-28T17:43:32.037674Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 187MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model: Attention U-Net++ with resnet34 encoder and SCSE attention\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [08:55<00:00,  2.83it/s, loss=0.115, IoU=0.872]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8619\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [08:59<00:00,  2.81it/s, loss=0.094, IoU=0.888] \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8841\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [08:59<00:00,  2.81it/s, loss=0.0858, IoU=0.897]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [08:59<00:00,  2.81it/s, loss=0.0865, IoU=0.898]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9026\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [08:58<00:00,  2.82it/s, loss=0.0871, IoU=0.897]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9072\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/782100786.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0782, Test IoU: 0.9068\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## DeepLabV3+","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    # Initialize DeepLabV3+ model\n    model = smp.DeepLabV3Plus(\n        encoder_name=\"resnet34\",      # Using ResNet34 for faster training\n        encoder_weights=\"imagenet\",    # Using pretrained weights\n        in_channels=3,                # RGB images\n        classes=1,                    # Binary segmentation\n    ).to(device)\n    \n    print(f\"Model: DeepLabV3Plus with {encoder_name} encoder\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:18:01.717536Z","iopub.execute_input":"2024-11-24T19:18:01.718313Z","iopub.status.idle":"2024-11-24T19:35:44.200897Z","shell.execute_reply.started":"2024-11-24T19:18:01.718253Z","shell.execute_reply":"2024-11-24T19:35:44.199829Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\nModel: DeepLabV3Plus with resnet34 encoder\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [03:12<00:00,  7.89it/s, loss=0.116, IoU=0.874]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8915\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [03:11<00:00,  7.91it/s, loss=0.0941, IoU=0.89] \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8777\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [03:11<00:00,  7.91it/s, loss=0.0904, IoU=0.892]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8919\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [03:11<00:00,  7.92it/s, loss=0.0871, IoU=0.896]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8986\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [03:11<00:00,  7.93it/s, loss=0.0833, IoU=0.899]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8962\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1907528388.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0861, Test IoU: 0.8986\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## MANet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    # Initialize DeepLabV3+ model\n    model = smp.MAnet(\n        encoder_name=\"resnet34\",        # Using ResNet34 as encoder\n        encoder_weights=\"imagenet\",      # Using pretrained weights\n        in_channels=3,                   # RGB images\n        classes=1,                       # Binary segmentation\n        decoder_channels=(256, 128, 64, 32, 16),  # Decoder channel sizes\n    ).to(device)\n    \n    print(f\"Model: MA-Net with {encoder_name} encoder\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:35:53.289792Z","iopub.execute_input":"2024-11-24T19:35:53.290141Z","iopub.status.idle":"2024-11-24T19:55:32.800239Z","shell.execute_reply.started":"2024-11-24T19:35:53.290109Z","shell.execute_reply":"2024-11-24T19:55:32.799210Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\nModel: MA-Net with resnet34 encoder\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [03:34<00:00,  7.06it/s, loss=0.121, IoU=0.871]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8787\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [03:35<00:00,  7.06it/s, loss=0.0919, IoU=0.891]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8988\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [03:34<00:00,  7.06it/s, loss=0.0957, IoU=0.889]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8979\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [03:34<00:00,  7.07it/s, loss=0.0868, IoU=0.897]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9004\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [03:34<00:00,  7.08it/s, loss=0.0838, IoU=0.9]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.5829\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/202613861.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0858, Test IoU: 0.9001\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## PAN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    # Initialize DeepLabV3+ model\n    # PAN (Pyramid Attention Network)\n    model = smp.PAN(\n        encoder_name=\"resnet34\",\n        encoder_weights=\"imagenet\",\n        in_channels=3,\n        classes=1,\n        decoder_channels=256\n    )\n\n    model = model.to(device)\n    \n    print(f\"Model: Pyramid attention network with {encoder_name} encoder\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:02:08.958032Z","iopub.execute_input":"2024-12-01T17:02:08.958785Z","iopub.status.idle":"2024-12-01T17:21:07.859878Z","shell.execute_reply.started":"2024-12-01T17:02:08.958751Z","shell.execute_reply":"2024-12-01T17:21:07.858826Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 187MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model: Pyramid attention network with resnet34 encoder\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [03:13<00:00,  7.85it/s, loss=0.112, IoU=0.875]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8122\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [03:23<00:00,  7.46it/s, loss=0.0889, IoU=0.894]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8733\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [03:23<00:00,  7.44it/s, loss=0.0868, IoU=0.897]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [03:23<00:00,  7.46it/s, loss=0.0834, IoU=0.901]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9031\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [03:22<00:00,  7.48it/s, loss=0.0826, IoU=0.9]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.9033\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/4292876771.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0809, Test IoU: 0.9031\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## FPN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Dataset paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n    \n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset, \n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # Model configuration\n    encoder_name = \"resnet34\"\n    \n    # Initialize Attention U-Net model\n    # Initialize DeepLabV3+ model\n    # PAN (Pyramid Attention Network)\n    # FPN (Feature Pyramid Network)\n    model = smp.FPN(\n        encoder_name=\"resnet34\",\n        encoder_weights=\"imagenet\",\n        in_channels=3,\n        classes=1,\n    )\n\n    model = model.to(device)\n    \n    print(f\"Model: Feature Pyramid Network with {encoder_name} encoder\")\n    \n    # Initialize loss and optimizer\n    criterion = smp.losses.DiceLoss('binary')\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:46:13.417172Z","iopub.execute_input":"2024-12-01T17:46:13.417570Z","iopub.status.idle":"2024-12-01T18:01:07.885664Z","shell.execute_reply.started":"2024-12-01T17:46:13.417534Z","shell.execute_reply":"2024-12-01T18:01:07.884580Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset splits: Train=12142, Val=2602, Test=2603\nModel: Feature Pyramid Network with resnet34 encoder\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 1518/1518 [02:37<00:00,  9.66it/s, loss=0.107, IoU=0.878]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8939\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 1518/1518 [02:36<00:00,  9.70it/s, loss=0.1, IoU=0.886]   \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8884\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 1518/1518 [02:36<00:00,  9.67it/s, loss=0.0859, IoU=0.897]\n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8938\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 1518/1518 [02:37<00:00,  9.61it/s, loss=0.0839, IoU=0.9]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8925\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 1518/1518 [02:36<00:00,  9.72it/s, loss=0.0832, IoU=0.9]  \n","output_type":"stream"},{"name":"stdout","text":"Validation IoU: 0.8804\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/4272857622.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0939, Test IoU: 0.8935\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## All model definitions","metadata":{}},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\n# UNet\nunet_model = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=(256, 128, 64, 32, 16),  # default decoder channels\n)\n\n# UNet++\nunetpp_model = smp.UnetPlusPlus(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=(256, 128, 64, 32, 16),\n    decoder_attention_type=\"scse\"  # Optional attention\n)\n\n# MANet\nmanet_model = smp.MAnet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=(256, 128, 64, 32, 16),\n    decoder_attention_type=\"scse\"\n)\n\n# LinkNet\nlinknet_model = smp.Linknet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=(256, 128, 64, 32, 16)\n)\n\n# FPN (Feature Pyramid Network)\nfpn_model = smp.FPN(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=(256, 128, 64, 32, 16)  # can be adjusted\n)\n\n# PSPNet (Pyramid Scene Parsing Network)\npsp_model = smp.PSPNet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    psp_out_channels=512,  # number of channels in PSP block\n    psp_use_batchnorm=True\n)\n\n# PAN (Pyramid Attention Network)\npan_model = smp.PAN(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=(256, 128, 64, 32, 16)\n)\n\n# DeepLabV3\ndeeplabv3_model = smp.DeepLabV3(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=256  # number of channels in decoder\n)\n\n# DeepLabV3+\ndeeplabv3plus_model = smp.DeepLabV3Plus(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    decoder_channels=256,\n    decoder_atrous_rates=(12, 24, 36)  # atrous rates for ASPP module\n)\n\n# Common encoder options (can be used with any model above):\ncommon_encoders = [\n    \"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\",\n    \"resnext50_32x4d\", \"resnext101_32x8d\",\n    \"timm-efficientnet-b0\", \"timm-efficientnet-b1\", \"timm-efficientnet-b2\",\n    \"timm-efficientnet-b3\", \"timm-efficientnet-b4\", \"timm-efficientnet-b5\",\n    \"timm-efficientnet-b6\", \"timm-efficientnet-b7\",\n    \"densenet121\", \"densenet169\", \"densenet201\",\n    \"dpn68\", \"dpn98\", \"dpn131\",\n    \"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\",\n    \"mobilenet_v2\"\n]\n\n# Optional decoder attention types (for models that support it):\nattention_types = [\n    \"scse\",       # Concurrent Spatial and Channel Squeeze & Excitation\n    \"cbam\",       # Convolutional Block Attention Module\n    \"eca\",        # Efficient Channel Attention\n    None          # No attention\n]\n\n# Example of how to check available encoders for a specific architecture\navailable_encoders = smp.Unet.get_encoder_names()\n# print(available_encoders)  # Uncomment to see all available encoders\n\n# Example of getting model with specific encoder\nmodel_with_specific_encoder = smp.Unet(\n    encoder_name=\"efficientnet-b0\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}