{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9790462,"sourceType":"datasetVersion","datasetId":5999150}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"#Run this cell if you are using collab.\n!pip install opendatasets\n!pip install pandas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:13:38.954869Z","iopub.execute_input":"2024-12-07T06:13:38.955247Z","iopub.status.idle":"2024-12-07T06:13:57.145649Z","shell.execute_reply.started":"2024-12-07T06:13:38.955213Z","shell.execute_reply":"2024-12-07T06:13:57.144666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport segmentation_models_pytorch as smp\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:14.666034Z","iopub.execute_input":"2024-12-07T06:14:14.666372Z","iopub.status.idle":"2024-12-07T06:14:14.671768Z","shell.execute_reply.started":"2024-12-07T06:14:14.666327Z","shell.execute_reply":"2024-12-07T06:14:14.670871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Class Definitions","metadata":{}},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class HuronDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = Path(image_dir)\n        self.mask_dir = Path(mask_dir)\n        self.transform = transform\n        self.mask_transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor()\n        ])\n        self.images = sorted(os.listdir(image_dir))\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.image_dir / self.images[idx]\n        mask_path = self.mask_dir / self.images[idx]\n\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n\n        if self.transform:\n            image = self.transform(image)\n        mask = self.mask_transform(mask)\n\n        # Ensure mask is binary\n        mask = (mask > 0.5).float()\n\n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:19.324908Z","iopub.execute_input":"2024-12-07T06:14:19.325235Z","iopub.status.idle":"2024-12-07T06:14:19.331871Z","shell.execute_reply.started":"2024-12-07T06:14:19.325206Z","shell.execute_reply":"2024-12-07T06:14:19.331028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Eval Metrics","metadata":{}},{"cell_type":"code","source":"def calculate_iou(outputs, targets, threshold=0.5):\n    outputs = (outputs > threshold).float()\n    targets = targets.float()\n\n    intersection = (outputs * targets).sum(dim=(1, 2))\n    union = outputs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) - intersection\n\n    iou = (intersection + 1e-6) / (union + 1e-6)\n    return iou.mean()\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_iou = 0\n    num_batches = len(dataloader)\n\n    with torch.no_grad():\n        for images, masks in dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            iou = calculate_iou(outputs, masks)\n\n            total_loss += loss.item()\n            total_iou += iou.item()\n\n    return total_loss / num_batches, total_iou / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:24.871033Z","iopub.execute_input":"2024-12-07T06:14:24.871397Z","iopub.status.idle":"2024-12-07T06:14:24.877909Z","shell.execute_reply.started":"2024-12-07T06:14:24.871367Z","shell.execute_reply":"2024-12-07T06:14:24.877064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs):\n    best_val_iou = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_iou = 0.0\n\n        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n            for images, masks in pbar:\n                images = images.to(device)\n                masks = masks.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                iou = calculate_iou(outputs, masks)\n\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n                running_iou += iou.item()\n\n                pbar.set_postfix({\n                    'loss': running_loss / (pbar.n + 1),\n                    'IoU': running_iou / (pbar.n + 1)\n                })\n\n        # Validate\n        model.eval()\n        val_iou = 0\n        with torch.no_grad():\n            for images, masks in val_loader:\n                images = images.to(device)\n                masks = masks.to(device)\n                outputs = model(images)\n                val_iou += calculate_iou(outputs, masks).item()\n\n        val_iou /= len(val_loader)\n        print(f'Validation IoU: {val_iou:.4f}')\n\n        if val_iou > best_val_iou:\n            best_val_iou = val_iou\n            torch.save(model.state_dict(), 'best_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:28.241715Z","iopub.execute_input":"2024-12-07T06:14:28.242546Z","iopub.status.idle":"2024-12-07T06:14:28.253808Z","shell.execute_reply.started":"2024-12-07T06:14:28.242497Z","shell.execute_reply":"2024-12-07T06:14:28.252911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"def visualize_results(model, test_loader, device, num_images=3):\n    model.eval()\n\n    # Get some test images\n    test_images, test_masks = next(iter(test_loader))\n\n    with torch.no_grad():\n        test_images = test_images.to(device)\n        predictions = model(test_images)\n        predictions = (predictions > 0.5).float()\n\n    # Convert tensors to numpy for visualization\n    test_images = test_images.cpu()\n    test_masks = test_masks.cpu()\n    predictions = predictions.cpu()\n\n    # Plot results\n    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5*num_images))\n\n    for idx in range(num_images):\n        # Original image\n        img = test_images[idx].permute(1, 2, 0)\n        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n        img = img.numpy()\n        img = np.clip(img, 0, 1)\n\n        # Ground truth mask\n        mask = test_masks[idx].squeeze().numpy()\n\n        # Predicted mask\n        pred = predictions[idx].squeeze().numpy()\n\n        # Plot\n        axes[idx, 0].imshow(img)\n        axes[idx, 0].set_title('Original Image')\n        axes[idx, 0].axis('off')\n\n        axes[idx, 1].imshow(mask, cmap='gray')\n        axes[idx, 1].set_title('Ground Truth Mask')\n        axes[idx, 1].axis('off')\n\n        axes[idx, 2].imshow(pred, cmap='gray')\n        axes[idx, 2].set_title('Predicted Mask')\n        axes[idx, 2].axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:34.489547Z","iopub.execute_input":"2024-12-07T06:14:34.490249Z","iopub.status.idle":"2024-12-07T06:14:34.497813Z","shell.execute_reply.started":"2024-12-07T06:14:34.490218Z","shell.execute_reply":"2024-12-07T06:14:34.496988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loss Metric","metadata":{}},{"cell_type":"code","source":"class CombinedLoss(nn.Module):\n    def __init__(self, loss1, loss2, weight1=1.0, weight2=1.0):\n        super(CombinedLoss, self).__init__()\n        self.loss1 = loss1\n        self.loss2 = loss2\n        self.weight1 = weight1\n        self.weight2 = weight2\n\n    def forward(self, y_pred, y_true):\n        return self.weight1 * self.loss1(y_pred, y_true) + self.weight2 * self.loss2(y_pred, y_true)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:38.480382Z","iopub.execute_input":"2024-12-07T06:14:38.481171Z","iopub.status.idle":"2024-12-07T06:14:38.486034Z","shell.execute_reply.started":"2024-12-07T06:14:38.481138Z","shell.execute_reply":"2024-12-07T06:14:38.484953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"### Kaggle Setup (2xT4 GPUs)\nSince we have dual GPUs, we encorporated Parallel Training.","metadata":{}},{"cell_type":"code","source":"import random\nimport csv\nimport wandb\nfrom pathlib import Path\n\ndef run_tuning_kaggle():\n    #We use WandB to help keep track of and analyze the tunning\n    wandb.init(project=\"huron_tuning_kaggle\", mode=\"offline\") #initialize WandB\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    #Set the paths\n    image_dir = '/kaggle/input/huron-dataset/Sliced_Images'\n    mask_dir = '/kaggle/input/huron-dataset/Sliced_masks'\n\n    #Transform the images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])])\n\n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n\n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n\n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n\n    #Here we define the search space for the tuning.\n    search_space = {\n        \"learning_rate\": [1e-4, 5e-4, 1e-3],\n        \"weight_decay\": [0.0, 0.01, 0.1],\n        \"batch_size\": [8, 16, 32],\n        \"num_epochs\": [10, 15, 20]}\n\n    #Setup an excel file to store our logs.\n    log_path = Path('./kaggle_results/tuning_log.csv')\n    log_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(log_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"trial_id\", \"learning_rate\", \"batch_size\", \"weight_decay\",\"num_epochs\", \"test_loss\", \"test_iou\"])\n\n    #We perform tuning by randomly picking the search space.\n    num_trials = 20\n    for trial_id in range(1, num_trials + 1):\n        lr = random.choice(search_space[\"learning_rate\"])\n        wd = random.choice(search_space[\"weight_decay\"])\n        batch_size = random.choice(search_space[\"batch_size\"])\n        num_epochs = random.choice(search_space[\"num_epochs\"])\n\n        #Update WandB config\n        wandb.config.update({\"trial_id\": trial_id, \"learning_rate\": lr, \"weight_decay\": wd, \"batch_size\": batch_size,\"num_epochs\": num_epochs})\n\n        #Update the dataloaders with new batch size\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n        test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n        # Initialize U-Net++\n        model = smp.UnetPlusPlus(\n            encoder_name=\"efficientnet-b4\",\n            encoder_weights=\"imagenet\",\n            in_channels=3,\n            classes=1\n        ).to(device)\n\n        #We are going to enable DataParallel to make use of kaggle's dual GPUs(T4).\n        model = torch.nn.DataParallel(model)\n\n        # Initialize loss and optimizer\n        criterion = CombinedLoss(\n            smp.losses.DiceLoss(mode='binary'),\n            nn.BCEWithLogitsLoss()\n        )\n        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n\n        #Set up Trial\n        print(f\"Trial {trial_id}: LR={lr}, Batch Size={batch_size}, Weight Decay={wd}, Epochs={num_epochs}\")\n        best_test_iou = 0 #holds the best iou found through the epochs.\n        patience = 2  #stops epochs when IoU stops improving after 2 iterations.\n        no_improve_epochs = 0 #stores the count at which IoU is not improving.\n\n        for epoch in range(3, num_epochs + 1):\n            print(f\"Number of Epochs:{epoch}\")\n            # Train and validate model\n            train_model(model, train_loader, val_loader, criterion, optimizer, device,num_epochs=epoch)\n            # Evaluate on Test set\n            test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n            if test_iou > best_test_iou:\n                best_test_iou = test_iou\n                no_improve_epochs = 0\n            else:\n                no_improve_epochs += 1\n\n            if no_improve_epochs >= patience:\n                print(\"Early stopping triggered.\")\n                break\n        \n        print(f\"Trial {trial_id} Results: Test Loss={test_loss:.4f}, Test IoU={best_test_iou:.4f}\")\n\n        #Log results to WandB and Excel file\n        wandb.log({\"test_loss\": test_loss, \"test_iou\": best_test_iou})\n        with open(log_path, mode='a', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow([trial_id, lr, batch_size, wd,num_epochs, test_loss, best_test_iou])\n\n    print(\"Tuning complete. Results saved to:\", log_path)\n\n#this runs the function\nif __name__ == \"__main__\":\n    run_tuning_kaggle()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:14:44.391507Z","iopub.execute_input":"2024-12-07T06:14:44.391929Z","iopub.status.idle":"2024-12-07T06:15:14.957824Z","shell.execute_reply.started":"2024-12-07T06:14:44.391851Z","shell.execute_reply":"2024-12-07T06:15:14.956515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Colab Setup (A100 GPU)\nSince we are using a much stringer GPU, higher batch sizes have been used to train our model.","metadata":{}},{"cell_type":"code","source":"import random\nimport csv\nimport wandb\nfrom pathlib import Path\n\ndef run_tuning_colab():\n    # We use WandB to help keep track of and analyze the tuning\n    wandb.init(project=\"huron_tuning_colab\", mode=\"offline\")  # Initialize WandB\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Set the paths\n    image_dir = '/content/huron-dataset/Sliced_Images'\n    mask_dir = '/content/huron-dataset/Sliced_masks'\n\n    # Transform the images\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])])\n\n    # Create full dataset\n    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n\n    # Calculate splits\n    total_size = len(full_dataset)\n    train_size = int(0.7 * total_size)\n    val_size = int(0.15 * total_size)\n    test_size = total_size - train_size - val_size\n\n    # Split dataset\n    train_dataset, val_dataset, test_dataset = random_split(\n        full_dataset,\n        [train_size, val_size, test_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n    # Here we define the search space for the tuning\n    search_space = {\n        \"learning_rate\": [5e-5, 1e-4, 5e-4],\n        \"weight_decay\": [0.0, 0.01, 0.1],\n        \"batch_size\": [16, 32, 64],\n        \"num_epochs\": [10, 15, 20]}\n\n    # Setup an excel file to store our logs\n    log_path = Path('./colab_results/tuning_log.csv')\n    log_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(log_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"trial_id\", \"learning_rate\", \"batch_size\", \"weight_decay\", \"num_epochs\", \"test_loss\", \"test_iou\"])\n\n    # We perform tuning by randomly picking the search space\n    num_trials = 20\n    for trial_id in range(1, num_trials + 1):\n        lr = random.choice(search_space[\"learning_rate\"])\n        wd = random.choice(search_space[\"weight_decay\"])\n        batch_size = random.choice(search_space[\"batch_size\"])\n        num_epochs = random.choice(search_space[\"num_epochs\"])\n\n        # Update WandB config\n        wandb.config.update({\"trial_id\": trial_id, \"learning_rate\": lr, \"weight_decay\": wd, \"batch_size\": batch_size, \"num_epochs\": num_epochs})\n\n        # Update the dataloaders with new batch size\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n        # Initialize U-Net++\n        model = smp.UnetPlusPlus(\n            encoder_name=\"efficientnet-b4\",\n            encoder_weights=\"imagenet\",\n            in_channels=3,\n            classes=1\n        ).to(device)\n\n        # Single GPU setup, no DataParallel needed\n        print(f\"Trial {trial_id}: LR={lr}, Batch Size={batch_size}, Weight Decay={wd}, Epochs={num_epochs}\")\n        best_test_iou = 0  # Holds the best IoU found through the epochs\n        patience = 2  # Stops epochs when IoU stops improving after 2 iterations\n        no_improve_epochs = 0  # Stores the count at which IoU is not improving\n\n        for epoch in range(3, num_epochs + 1):\n            print(f\"Number of Epochs: {epoch}\")\n            # Train and validate model\n            train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=epoch)\n            # Evaluate on Test set\n            test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n            if test_iou > best_test_iou:\n                best_test_iou = test_iou\n                no_improve_epochs = 0\n            else:\n                no_improve_epochs += 1\n\n            if no_improve_epochs >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n        print(f\"Trial {trial_id} Results: Test Loss={test_loss:.4f}, Test IoU={best_test_iou:.4f}\")\n\n        # Log results to WandB and Excel file\n        wandb.log({\"test_loss\": test_loss, \"test_iou\": best_test_iou})\n        with open(log_path, mode='a', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow([trial_id, lr, batch_size, wd, num_epochs, test_loss, best_test_iou])\n\n    print(\"Tuning complete. Results saved to:\", log_path)\n\n\n# This runs the function\nif __name__ == \"__main__\":\n    run_tuning_colab()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}