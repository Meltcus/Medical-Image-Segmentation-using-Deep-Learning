{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n"
      ],
      "metadata": {
        "id": "PPX-_zTZUaTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TIPU_fStdW1W",
        "outputId": "5bc69c6b-e24c-463e-e982-8122f6e7655a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.6)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D36ayyRh_R9a"
      },
      "source": [
        "you can use my kaggle credentials:\n",
        "\n",
        "nomielagarde\n",
        "\n",
        "d8e0f8ad13e663040a0d037b2208f1b6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0kT7vhvdcM4",
        "outputId": "2f8b91bd-e901-47fa-dec4-0d69c9f75f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: nomielagarde\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/nomielagarde/huron-dataset\n",
            "Downloading huron-dataset.zip to ./huron-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.35G/3.35G [02:58<00:00, 20.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/nomielagarde/huron-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vdGXDk9E_WgM",
        "outputId": "949560a6-0025-48b8-8bd2-3f78b0da2f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.26.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (11.0.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.16.0)\n",
            "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.5.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.0.2)\n",
            "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=64ad328e47fefd56dd1dc5a6e5cc1ed45212c12c2324eda7327042e1834c3ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=ce39da0c3d19f5430e3c79e9f0ceb7862037c8dc8a6559bdf38a5f303542e9be\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n"
          ]
        }
      ],
      "source": [
        "#installing smp libraru\n",
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import segmentation_models_pytorch as smp\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aOqCx9W9UmXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Definitions"
      ],
      "metadata": {
        "id": "I_CiR3DkUnq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "ocBkKe9eUqMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC6ZQZFM_cUz"
      },
      "outputs": [],
      "source": [
        "class HuronDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.mask_dir = Path(mask_dir)\n",
        "        self.transform = transform\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_dir / self.images[idx]\n",
        "        mask_path = self.mask_dir / self.images[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        mask = self.mask_transform(mask)\n",
        "\n",
        "        # Ensure mask is binary\n",
        "        mask = (mask > 0.5).float()\n",
        "\n",
        "        return image, mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval metrics"
      ],
      "metadata": {
        "id": "-XDV6a3vU4LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(outputs, targets, threshold=0.5):\n",
        "    outputs = (outputs > threshold).float()\n",
        "    targets = targets.float()\n",
        "\n",
        "    intersection = (outputs * targets).sum(dim=(1, 2))\n",
        "    union = outputs.sum(dim=(1, 2)) + targets.sum(dim=(1, 2)) - intersection\n",
        "\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    return iou.mean()\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_iou = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            iou = calculate_iou(outputs, masks)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_iou += iou.item()\n",
        "\n",
        "    return total_loss / num_batches, total_iou / num_batches\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, loss1, loss2, weight1=1.0, weight2=1.0):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.loss1 = loss1\n",
        "        self.loss2 = loss2\n",
        "        self.weight1 = weight1\n",
        "        self.weight2 = weight2\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return self.weight1 * self.loss1(y_pred, y_true) + self.weight2 * self.loss2(y_pred, y_true)\n"
      ],
      "metadata": {
        "id": "dE7HvzbMUxJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "JsVSY4-GVDYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(model, test_loader, device, num_images=3):\n",
        "    model.eval()\n",
        "\n",
        "    # Get some test images\n",
        "    test_images, test_masks = next(iter(test_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_images = test_images.to(device)\n",
        "        predictions = model(test_images)\n",
        "        predictions = (predictions > 0.5).float()\n",
        "\n",
        "    # Convert tensors to numpy for visualization\n",
        "    test_images = test_images.cpu()\n",
        "    test_masks = test_masks.cpu()\n",
        "    predictions = predictions.cpu()\n",
        "\n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5*num_images))\n",
        "\n",
        "    for idx in range(num_images):\n",
        "        # Original image\n",
        "        img = test_images[idx].permute(1, 2, 0)\n",
        "        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
        "        img = img.numpy()\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        # Ground truth mask\n",
        "        mask = test_masks[idx].squeeze().numpy()\n",
        "\n",
        "        # Predicted mask\n",
        "        pred = predictions[idx].squeeze().numpy()\n",
        "\n",
        "        # Plot\n",
        "        axes[idx, 0].imshow(img)\n",
        "        axes[idx, 0].set_title('Original Image')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(mask, cmap='gray')\n",
        "        axes[idx, 1].set_title('Ground Truth Mask')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        axes[idx, 2].imshow(pred, cmap='gray')\n",
        "        axes[idx, 2].set_title('Predicted Mask')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mfyZ_r6eVFBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "E9IvpazzVG8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5):\n",
        "    best_val_iou = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_iou = 0.0\n",
        "\n",
        "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
        "            for images, masks in pbar:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                iou = calculate_iou(outputs, masks)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                running_iou += iou.item()\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': running_loss / (pbar.n + 1),\n",
        "                    'IoU': running_iou / (pbar.n + 1)\n",
        "                })\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_iou = 0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "                outputs = model(images)\n",
        "                val_iou += calculate_iou(outputs, masks).item()\n",
        "\n",
        "        val_iou /= len(val_loader)\n",
        "        print(f'Validation IoU: {val_iou:.4f}')\n",
        "\n",
        "        if val_iou > best_val_iou:\n",
        "            best_val_iou = val_iou\n",
        "            torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "id": "4x_532odU8SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdamW"
      ],
      "metadata": {
        "id": "tKLtGTUIVL_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5UutffL_ijI",
        "outputId": "dd0d0f06-1195-4a6b-9dc5-4011fc0cd011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset splits: Train=12142, Val=2602, Test=2603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n",
            "100%|██████████| 74.4M/74.4M [00:02<00:00, 36.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: U-Net++ with efficientnet-b4 encoder and AdamW Optimizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 1518/1518 [02:46<00:00,  9.10it/s, loss=0.196, IoU=0.897]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 1518/1518 [02:43<00:00,  9.26it/s, loss=0.16, IoU=0.91]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 1518/1518 [02:43<00:00,  9.29it/s, loss=0.156, IoU=0.912]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.8559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 1518/1518 [02:43<00:00,  9.29it/s, loss=0.149, IoU=0.913]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 1518/1518 [02:43<00:00,  9.28it/s, loss=0.147, IoU=0.914]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9174\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-54a2804410d2>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1493, Test IoU: 0.9163\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Dataset paths\n",
        "    image_dir = '/content/huron-dataset/Sliced_Images'\n",
        "    mask_dir = '/content/huron-dataset/Sliced_masks'\n",
        "\n",
        "    # Transform for input images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n",
        "\n",
        "    # Calculate splits\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model configuration\n",
        "    encoder_name = \"efficientnet-b4\"\n",
        "\n",
        "    # Initialize U-Net model\n",
        "    model = smp.UnetPlusPlus(\n",
        "        encoder_name=encoder_name,        # try different encoders\n",
        "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n",
        "        in_channels=3,                  # model input channels\n",
        "        classes=1                      # model output channels\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # Initialize loss and optimizer\n",
        "    criterion = CombinedLoss(\n",
        "        smp.losses.DiceLoss(mode='binary'),\n",
        "        nn.BCEWithLogitsLoss())\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "    print(f\"Model: U-Net++ with {encoder_name} encoder and AdamW Optimizer\")\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating on test set...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp"
      ],
      "metadata": {
        "id": "4nHi_GpNVOdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Dataset paths\n",
        "    image_dir = '/content/huron-dataset/Sliced_Images'\n",
        "    mask_dir = '/content/huron-dataset/Sliced_masks'\n",
        "\n",
        "    # Transform for input images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n",
        "\n",
        "    # Calculate splits\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model configuration\n",
        "    encoder_name = \"efficientnet-b4\"\n",
        "\n",
        "    # Initialize U-Net model\n",
        "    model = smp.UnetPlusPlus(\n",
        "        encoder_name=encoder_name,        # try different encoders\n",
        "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n",
        "        in_channels=3,                  # model input channels\n",
        "        classes=1                      # model output channels\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # Initialize loss and optimizer\n",
        "    criterion = CombinedLoss(\n",
        "        smp.losses.DiceLoss(mode='binary'),\n",
        "        nn.BCEWithLogitsLoss())\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n",
        "\n",
        "    print(f\"Model: U-Net++ with {encoder_name} encoder and AdamW Optimizer\")\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating on test set...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTzf03bnv4fJ",
        "outputId": "4a139159-6b1c-4533-b3da-b216bddbb1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset splits: Train=12142, Val=2602, Test=2603\n",
            "Model: U-Net++ with efficientnet-b4 encoder and AdamW Optimizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.41it/s, loss=0.192, IoU=0.896]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.38it/s, loss=0.16, IoU=0.909]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.41it/s, loss=0.155, IoU=0.911]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.39it/s, loss=0.151, IoU=0.914]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 1518/1518 [02:42<00:00,  9.36it/s, loss=0.146, IoU=0.915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9169\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8c98671621a0>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1487, Test IoU: 0.9159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD"
      ],
      "metadata": {
        "id": "qo4v87_xVR0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Dataset paths\n",
        "    image_dir = '/content/huron-dataset/Sliced_Images'\n",
        "    mask_dir = '/content/huron-dataset/Sliced_masks'\n",
        "\n",
        "    # Transform for input images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = HuronDataset(image_dir, mask_dir, transform=transform)\n",
        "\n",
        "    # Calculate splits\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model configuration\n",
        "    encoder_name = \"efficientnet-b4\"\n",
        "\n",
        "    # Initialize U-Net model\n",
        "    model = smp.UnetPlusPlus(\n",
        "        encoder_name=encoder_name,        # try different encoders\n",
        "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights\n",
        "        in_channels=3,                  # model input channels\n",
        "        classes=1                      # model output channels\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # Initialize loss and optimizer\n",
        "    criterion = CombinedLoss(\n",
        "        smp.losses.DiceLoss(mode='binary'),\n",
        "        nn.BCEWithLogitsLoss())\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    print(f\"Model: U-Net++ with {encoder_name} encoder and AdamW Optimizer\")\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating on test set...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loss, test_iou = evaluate(model, test_loader, criterion, device)\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT3mnanvv-vH",
        "outputId": "63a3664b-80d6-469d-9ea6-8cfbf94ec861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset splits: Train=12142, Val=2602, Test=2603\n",
            "Model: U-Net++ with efficientnet-b4 encoder and AdamW Optimizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.40it/s, loss=0.225, IoU=0.883]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 1518/1518 [02:40<00:00,  9.46it/s, loss=0.174, IoU=0.903]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.37it/s, loss=0.164, IoU=0.906]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 1518/1518 [02:41<00:00,  9.41it/s, loss=0.159, IoU=0.909]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 1518/1518 [02:40<00:00,  9.45it/s, loss=0.154, IoU=0.91]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation IoU: 0.9076\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1cff252de3c9>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1591, Test IoU: 0.9076\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}